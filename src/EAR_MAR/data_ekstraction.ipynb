{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743a4447-3be6-4865-a3d7-f8aeb4a457b9",
   "metadata": {},
   "source": [
    " # Analisis EAR "
   ]
  },
  {
   "cell_type": "raw",
   "id": "219b9607-9ef9-4c34-b694-9453bc0e6713",
   "metadata": {},
   "source": [
    "Pada bagian ini divisisualisakan nilai ear dengan tujuan untuk melihat dimana nilai ear yang\n",
    "menunjukkan mata sedang tertutup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328f3319",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39637f51-fbe9-41c7-a1f6-402bbb587e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Deteksi wajah\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     46\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghitung Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Fungsi untuk mendeteksi wajah dan landmark\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Folder tempat video disimpan\n",
    "video_folder = \"D:\\TA-main\\data\"\n",
    "\n",
    "# Daftar video dalam folder\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith('.avi')]\n",
    "\n",
    "ear_values_all = []  # List untuk menyimpan nilai EAR dari semua video\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    ear_values = []  # List untuk menyimpan nilai EAR dari setiap frame\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Ubah ke grayscale untuk deteksi\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Deteksi wajah\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Ambil koordinat mata\n",
    "            left_eye = []\n",
    "            right_eye = []\n",
    "            for n in range(36, 42):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                left_eye.append((x, y))\n",
    "            for n in range(42, 48):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                right_eye.append((x, y))\n",
    "\n",
    "            # Hitung EAR untuk setiap mata\n",
    "            left_ear = eye_aspect_ratio(np.array(left_eye))\n",
    "            right_ear = eye_aspect_ratio(np.array(right_eye))\n",
    "            ear = (left_ear + right_ear) / 2\n",
    "            ear_values.append(ear)\n",
    "            \n",
    "            # Gambar garis di sekitar mata (opsional, hanya untuk tujuan visualisasi)\n",
    "            cv2.polylines(frame, [np.array(left_eye)], True, (0, 255, 0), 1)\n",
    "            cv2.polylines(frame, [np.array(right_eye)], True, (0, 255, 0), 1)\n",
    "\n",
    "        # Tampilkan frame (opsional, hanya untuk tujuan visualisasi)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Tutup video\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Gabungkan nilai EAR dari video saat ini ke dalam list utama\n",
    "    ear_values_all.extend(ear_values)\n",
    "\n",
    "# Hitung statistik EAR\n",
    "mean_ear = np.mean(ear_values_all)\n",
    "min_ear = np.min(ear_values_all)\n",
    "max_ear = np.max(ear_values_all)\n",
    "\n",
    "# Plot grafik EAR\n",
    "frame_indices = np.arange(1, len(ear_values_all) + 1)\n",
    "plt.plot(frame_indices, ear_values_all)\n",
    "plt.axhline(y=mean_ear, color='r', linestyle='--', label=f'Rata-rata: {mean_ear:.2f}')\n",
    "plt.axhline(y=min_ear, color='g', linestyle='--', label=f'Minimum: {min_ear:.2f}')\n",
    "plt.axhline(y=max_ear, color='b', linestyle='--', label=f'Maksimum: {max_ear:.2f}')\n",
    "plt.title('Grafik Eye Aspect Ratio (EAR) terhadap Waktu')\n",
    "plt.xlabel('Nomor Frame')\n",
    "plt.ylabel('EAR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0870a-4988-45e7-90a1-33b35ba6f4e5",
   "metadata": {},
   "source": [
    "## Percobaan Nilai EAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a91945-9c0d-46ea-b75f-ee41b46983fb",
   "metadata": {},
   "source": [
    "Pada kode dibawah ini, mengestrakan data dalam bentuk vidio menjadi gambar berdasarkan nilai ear.\n",
    "dengan tujuan untuk mencari nilai ear yang sesuai. pada bagian ini akan di coba beberapa nilai ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a45543c7-11e4-4ed6-8c8a-4c4264b8468c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     43\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 44\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "\n",
    "\n",
    "def calculate_eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    EAR = (A + B) / (2.0 * C)\n",
    "    return EAR\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "eye_aspect_ratio_threshold = 0.38\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Menyimpan path folder video\n",
    "video_folder = \"D:\\TA-main\\data\"\n",
    "\n",
    "# Looping semua file video dalam folder\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Video {filename} telah selesai diputar.\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "                \n",
    "            for face in faces:\n",
    "                shape = predictor(gray, face)\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "                left_eye = shape[lStart:lEnd]\n",
    "                right_eye = shape[rStart:rEnd]\n",
    "                leftEAR = calculate_eye_aspect_ratio(left_eye)\n",
    "                rightEAR = calculate_eye_aspect_ratio(right_eye)\n",
    "                EAR = (leftEAR + rightEAR) / 2.0\n",
    "           \n",
    "                \n",
    "                #menggambarkan daerah mata\n",
    "                cv2.polylines(frame, [np.array(left_eye)], True, (0, 0, 255), 1)\n",
    "                cv2.polylines(frame, [np.array(right_eye)], True, (0, 0, 255), 1)\n",
    "                cv2.putText(frame, \"EAR: {:.2f}\".format(EAR), (150, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "           \n",
    "\n",
    "                if EAR > eye_aspect_ratio_threshold:\n",
    "                    output_folder = \"tidak mengantuk\"\n",
    "                elif EAR < eye_aspect_ratio_threshold:\n",
    "                    output_folder = \"mengantuk\"\n",
    "                \n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "            \n",
    "\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = os.path.join(output_folder, f\"{filename}_frame_{counter}.jpg\")\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                counter += 1\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee1b19-ff7d-40be-9c24-9d65357a4f7e",
   "metadata": {},
   "source": [
    "# Analisis MAR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e2c0a-2fd3-46c1-8a5c-264e437a2d7c",
   "metadata": {},
   "source": [
    "Pada bagian ini divisisualisakan nilai mar dengan tujuan untuk melihat dimana nilai mar yang\n",
    "menunjukkan kondisi mulut sedang tertutup atau terbuka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ebaa760-7072-402a-8ade-f2d725ca1fa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Deteksi wajah\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     45\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Fungsi untuk menghitung Mouth Aspect Ratio (MAR)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = np.linalg.norm(mouth[3] - mouth[9])\n",
    "    B = np.linalg.norm(mouth[0] - mouth[6])\n",
    "    mar = A / B\n",
    "    return mar\n",
    "\n",
    "# Fungsi untuk mendeteksi wajah dan landmark\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Folder tempat video disimpan\n",
    "video_folder = \"D:\\TA-main\\data\"\n",
    "\n",
    "# Daftar video dalam folder\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith('.avi')]\n",
    "\n",
    "mar_values_all = []  # List untuk menyimpan nilai MAR dari semua video\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    mar_values = []  # List untuk menyimpan nilai MAR dari setiap frame\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Ubah ke grayscale untuk deteksi\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Deteksi wajah\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "\n",
    "            # Ambil koordinat mulut\n",
    "            mouth = []\n",
    "            for n in range(48, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                mouth.append((x, y))\n",
    "\n",
    "            # Hitung MAR untuk mulut\n",
    "            mar = mouth_aspect_ratio(np.array(mouth))\n",
    "            mar_values.append(mar)\n",
    "            \n",
    "            # Gambar garis di sekitar mulut (opsional, hanya untuk tujuan visualisasi)\n",
    "            cv2.polylines(frame, [np.array(mouth)], True, (0, 255, 0), 1)\n",
    "            \n",
    "        # Tampilkan frame (opsional, hanya untuk tujuan visualisasi)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Tutup video\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Gabungkan nilai MAR dari video saat ini ke dalam list utama\n",
    "    mar_values_all.extend(mar_values)\n",
    "\n",
    "# Hitung statistik MAR\n",
    "mean_mar = np.mean(mar_values_all)\n",
    "min_mar = np.min(mar_values_all)\n",
    "max_mar = np.max(mar_values_all)\n",
    "\n",
    "# Plot grafik MAR\n",
    "frame_indices = np.arange(1, len(mar_values_all) + 1)\n",
    "plt.plot(frame_indices, mar_values_all)\n",
    "plt.axhline(y=mean_mar, color='r', linestyle='--', label=f'Rata-rata: {mean_mar:.2f}')\n",
    "plt.axhline(y=min_mar, color='g', linestyle='--', label=f'Minimum: {min_mar:.2f}')\n",
    "plt.axhline(y=max_mar, color='b', linestyle='--', label=f'Maksimum: {max_mar:.2f}')\n",
    "plt.title('Grafik Mouth Aspect Ratio (MAR) terhadap Waktu')\n",
    "plt.xlabel('Nomor Frame')\n",
    "plt.ylabel('MAR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e5b32-ba21-4049-9c91-f5bba76b62ff",
   "metadata": {},
   "source": [
    "## Percobaan Nilai MAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1624e3-eaee-45ff-ae34-ef7e4f7b8c8c",
   "metadata": {},
   "source": [
    "Pada kode dibawah ini, mengestrakan data dalam bentuk vidio menjadi gambar berdasarkan nilai mar.\n",
    "dengan tujuan untuk mencari nilai mar yang sesuai. pada bagian ini akan di coba beberapa nilai mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61613c10-d907-4984-b1a4-eff818ddd2c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     37\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 38\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     40\u001b[0m     shape \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "\n",
    "def calculate_lip_aspect_ratio(lips):\n",
    "    dist1 = dist.euclidean(lips[3], lips[9]) \n",
    "    dist2 = dist.euclidean(lips[0], lips[6]) \n",
    "    lar = float(dist1/dist2)\n",
    "    return lar\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "lip_aspect_ratio_threshold = 0.40\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Menyimpan path folder video\n",
    "video_folder = \"D:\\TA-main\\data\"\n",
    "\n",
    "# Looping semua file video dalam folder\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Video {filename} telah selesai diputar.\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "            for face in faces:\n",
    "                shape = predictor(gray, face)\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "                lips = shape[48:68]\n",
    "                lar = calculate_lip_aspect_ratio(lips)\n",
    "                \n",
    "                \n",
    "           \n",
    "\n",
    "                if lar > lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"menguap\"\n",
    "                elif lar < lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"tidak menguap\"\n",
    "                \n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "            \n",
    "\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = os.path.join(output_folder, f\"{filename}_frame_{counter}.jpg\")\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                counter += 1\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453dcc9-869f-4c63-b670-dc39729fbd8f",
   "metadata": {},
   "source": [
    "# Ektraksi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccee9dd-1d5a-4896-b34c-7dd78d6bb66e",
   "metadata": {},
   "source": [
    "Setelah nilai terbaik dari ear dan mar, selanjutnya pada bagian ini data video akan di ekstrak menjadi gambar\n",
    "berdasarkan kedua parameter tersebut menjadi 4 kelas, yang merupakan kombinasi dari mar dan ear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe36004a-135a-495d-95be-3c268ece5512",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     47\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 48\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     50\u001b[0m     shape \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "def calculate_eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    EAR = (A + B) / (2.0 * C)\n",
    "    return EAR\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]\n",
    "\n",
    "def calculate_lip_aspect_ratio(lips):\n",
    "    dist1 = dist.euclidean(lips[3], lips[9]) \n",
    "    dist2 = dist.euclidean(lips[0], lips[6]) \n",
    "    lar = float(dist1/dist2)\n",
    "    return lar\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "eye_aspect_ratio_threshold = 0.20  \n",
    "lip_aspect_ratio_threshold = 0.50\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Menyimpan path folder video\n",
    "video_folder = \"D:\\TA-main\\data\"\n",
    "\n",
    "# Looping semua file video dalam folder\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith(\".avi\"):\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Video {filename} telah selesai diputar.\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "            for face in faces:\n",
    "                shape = predictor(gray, face)\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "                left_eye = shape[lStart:lEnd]\n",
    "                right_eye = shape[rStart:rEnd]\n",
    "                lips = shape[48:68]\n",
    "                leftEAR = calculate_eye_aspect_ratio(left_eye)\n",
    "                rightEAR = calculate_eye_aspect_ratio(right_eye)\n",
    "                EAR = (leftEAR + rightEAR) / 2.0\n",
    "                lar = calculate_lip_aspect_ratio(lips)\n",
    "                \n",
    "                # menggambarkan daerah bibir\n",
    "#                 cv2.putText(frame, \"LAR: {:.2f}\".format(lar), (315, 30),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "#                 cv2.polylines(frame, [np.array(lips)], True, (0, 255, 0), 1)\n",
    "                \n",
    "#                 #menggambarkan daerah mata\n",
    "#                 cv2.polylines(frame, [np.array(left_eye)], True, (0, 0, 255), 1)\n",
    "#                 cv2.polylines(frame, [np.array(right_eye)], True, (0, 0, 255), 1)\n",
    "#                 cv2.putText(frame, \"EAR: {:.2f}\".format(EAR), (150, 30),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "                \n",
    "           \n",
    "\n",
    "                if EAR < eye_aspect_ratio_threshold and lar > lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"mengantuk & menguap\"\n",
    "                elif EAR < eye_aspect_ratio_threshold and lar < lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"mengantuk & tidak menguap\"\n",
    "                elif EAR > eye_aspect_ratio_threshold and lar > lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"tidak mengantuk & menguap\"\n",
    "                elif EAR > eye_aspect_ratio_threshold and lar < lip_aspect_ratio_threshold:\n",
    "                    output_folder = \"tidak mengantuk & tidak menguap\"\n",
    "                \n",
    "                cv2.imshow(\"Face Detection\", frame)\n",
    "            \n",
    "\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = os.path.join(output_folder, f\"{filename}_frame_{counter}.jpg\")\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                counter += 1\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
